"""
LangChain Chat Models Gradio æ‡‰ç”¨ç¨‹å¼
æ•´åˆå¤šç¨® AI æ¨¡å‹ï¼ˆOllamaã€Geminiã€OpenAIã€Anthropicï¼‰çš„å°è©±ä»‹é¢
æ”¯æ´å°è©±è¨˜æ†¶ã€æ¨¡å‹åˆ‡æ›ã€å°è©±åŒ¯å‡ºç­‰åŠŸèƒ½
"""

import gradio as gr
import os
import json
import socket
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# LangChain imports
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class ChatModelsManager:
    """ç®¡ç†å¤šç¨® Chat Models çš„é¡åˆ¥"""
    
    def __init__(self):
        self.models = {}
        self.current_model = None
        self.conversation_history = []
        self.system_message = "ä½ æ˜¯ä¸€å€‹å‹å–„ä¸”æ¨‚æ–¼åŠ©äººçš„ AI åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
        
        # åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        try:
            # Ollama æ¨¡å‹
            self.models["Ollama (llama3.2)"] = ChatOllama(
                model="llama3.2:latest", 
                base_url="http://localhost:11434"
            )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– Ollama æ¨¡å‹: {e}")
        
        try:
            # Google Gemini æ¨¡å‹
            if os.getenv("GOOGLE_API_KEY"):
                self.models["Gemini (gemini-2.5-flash)"] = ChatGoogleGenerativeAI(
                    model="gemini-2.5-flash"
                )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– Gemini æ¨¡å‹: {e}")
        
        try:
            # OpenAI æ¨¡å‹
            if os.getenv("OPENAI_API_KEY"):
                self.models["OpenAI (gpt-4o-mini)"] = ChatOpenAI(
                    model="gpt-4o-mini"
                )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– OpenAI æ¨¡å‹: {e}")
        
        try:
            # Anthropic æ¨¡å‹
            if os.getenv("ANTHROPIC_API_KEY"):
                self.models["Anthropic (claude-3-5-sonnet)"] = ChatAnthropic(
                    model="claude-3-5-sonnet-latest"
                )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– Anthropic æ¨¡å‹: {e}")
        
        # è¨­å®šé è¨­æ¨¡å‹
        if self.models:
            self.current_model = list(self.models.keys())[0]
    
    def get_available_models(self) -> List[str]:
        """å–å¾—å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.models.keys())
    
    def set_model(self, model_name: str) -> str:
        """åˆ‡æ›æ¨¡å‹"""
        if model_name in self.models:
            self.current_model = model_name
            return f"âœ… å·²åˆ‡æ›è‡³ {model_name}"
        return f"âŒ æ¨¡å‹ {model_name} ä¸å¯ç”¨"
    
    def set_system_message(self, message: str):
        """è¨­å®šç³»çµ±è¨Šæ¯"""
        self.system_message = message
        return "âœ… ç³»çµ±è¨Šæ¯å·²æ›´æ–°"
    
    def clear_conversation(self):
        """æ¸…é™¤å°è©±æ­·å²"""
        self.conversation_history = []
        return "âœ… å°è©±æ­·å²å·²æ¸…é™¤"
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """å–å¾—å°è©±æ­·å²"""
        return self.conversation_history
    
    def add_message(self, role: str, content: str):
        """æ·»åŠ è¨Šæ¯åˆ°å°è©±æ­·å²"""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
    
    def chat(self, user_input: str, model_name: str = None) -> str:
        """é€²è¡Œå°è©±"""
        if not user_input.strip():
            return "è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."
        
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹æˆ–ç•¶å‰æ¨¡å‹
        selected_model = model_name if model_name and model_name in self.models else self.current_model
        
        if not selected_model or selected_model not in self.models:
            return "âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè«‹æª¢æŸ¥æ¨¡å‹è¨­å®š"
        
        try:
            # å»ºç«‹è¨Šæ¯åˆ—è¡¨
            messages = [SystemMessage(content=self.system_message)]
            
            # æ·»åŠ å°è©±æ­·å²
            for msg in self.conversation_history:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))
            
            # æ·»åŠ ç•¶å‰ç”¨æˆ¶è¼¸å…¥
            messages.append(HumanMessage(content=user_input))
            
            # å‘¼å«æ¨¡å‹
            model = self.models[selected_model]
            response = model.invoke(messages)
            
            # å–å¾—å›æ‡‰å…§å®¹
            if hasattr(response, 'content'):
                ai_response = response.content
            else:
                ai_response = str(response)
            
            # ä¿å­˜å°è©±
            self.add_message("user", user_input)
            self.add_message("assistant", ai_response)
            
            return ai_response
            
        except Exception as e:
            error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            return error_msg

# å»ºç«‹å…¨åŸŸçš„ ChatModelsManager å¯¦ä¾‹
chat_manager = ChatModelsManager()

def chat_function(message: str, history: List[Dict[str, str]], model_name: str) -> tuple:
    """Gradio èŠå¤©å‡½æ•¸"""
    if not message.strip():
        return history, ""
    
    # é€²è¡Œå°è©±
    response = chat_manager.chat(message, model_name)
    
    # æ›´æ–° Gradio æ­·å² (ä½¿ç”¨ messages æ ¼å¼)
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    
    return history, ""

def clear_chat():
    """æ¸…é™¤èŠå¤©"""
    chat_manager.clear_conversation()
    return [], ""

def update_system_message(system_msg: str):
    """æ›´æ–°ç³»çµ±è¨Šæ¯"""
    chat_manager.set_system_message(system_msg)
    return "âœ… ç³»çµ±è¨Šæ¯å·²æ›´æ–°"

def export_conversation():
    """åŒ¯å‡ºå°è©±æ­·å²"""
    history = chat_manager.get_conversation_history()
    if not history:
        return "âŒ æ²’æœ‰å°è©±æ­·å²å¯åŒ¯å‡º"
    
    # å»ºç«‹åŒ¯å‡ºå…§å®¹
    export_content = f"# å°è©±æ­·å²åŒ¯å‡º\n"
    export_content += f"åŒ¯å‡ºæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    export_content += f"ä½¿ç”¨æ¨¡å‹: {chat_manager.current_model}\n\n"
    
    for i, msg in enumerate(history, 1):
        role_emoji = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        export_content += f"## {i}. {role_emoji} {msg['role'].title()}\n"
        export_content += f"**æ™‚é–“**: {msg['timestamp']}\n"
        export_content += f"**å…§å®¹**: {msg['content']}\n\n"
    
    return export_content

def get_model_info():
    """å–å¾—æ¨¡å‹è³‡è¨Š"""
    available_models = chat_manager.get_available_models()
    current_model = chat_manager.current_model
    
    info = f"## ğŸ“Š æ¨¡å‹è³‡è¨Š\n\n"
    info += f"**ç•¶å‰æ¨¡å‹**: {current_model}\n\n"
    info += f"**å¯ç”¨æ¨¡å‹**:\n"
    for i, model in enumerate(available_models, 1):
        status = "ğŸŸ¢" if model == current_model else "âšª"
        info += f"{i}. {status} {model}\n"
    
    return info

# å»ºç«‹ Gradio ä»‹é¢
def create_gradio_interface():
    """å»ºç«‹ Gradio ä»‹é¢"""
    
    # å–å¾—å¯ç”¨æ¨¡å‹
    available_models = chat_manager.get_available_models()
    
    with gr.Blocks(
        title="LangChain Chat Models æ‡‰ç”¨ç¨‹å¼",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
            margin: auto !important;
        }
        
        /* èŠå¤©ä»‹é¢æ¨£å¼ */
        .chatbot {
            min-height: 500px !important;
            max-height: 700px !important;
        }
        
        /* å°è©±è¨Šæ¯æ¨£å¼ */
        .chat-message {
            padding: 12px 16px !important;
            margin: 8px 0 !important;
            border-radius: 12px !important;
            max-width: 100% !important;
            word-wrap: break-word !important;
            white-space: pre-wrap !important;
        }
        
        /* ç”¨æˆ¶è¨Šæ¯æ¨£å¼ */
        .chat-message.user {
            background-color: #e3f2fd !important;
            border-left: 4px solid #2196f3 !important;
            margin-left: 20% !important;
        }
        
        /* AI è¨Šæ¯æ¨£å¼ */
        .chat-message.assistant {
            background-color: #f3e5f5 !important;
            border-left: 4px solid #9c27b0 !important;
            margin-right: 20% !important;
        }
        
        /* è¼¸å…¥æ¡†æ¨£å¼ */
        .textbox {
            min-height: 60px !important;
        }
        
        /* æŒ‰éˆ•æ¨£å¼ */
        .btn {
            border-radius: 8px !important;
            padding: 8px 16px !important;
        }
        
        /* å³å´é¢æ¿æ¨£å¼ */
        .panel {
            background-color: #fafafa !important;
            border-radius: 12px !important;
            padding: 16px !important;
            margin: 8px !important;
        }
        
        /* éŸ¿æ‡‰å¼è¨­è¨ˆ */
        @media (min-width: 1200px) {
            .gradio-container {
                max-width: 1600px !important;
            }
        }
        
        @media (min-width: 1400px) {
            .gradio-container {
                max-width: 1800px !important;
            }
        }
        """
    ) as interface:
        
        gr.Markdown("""
        # ğŸ¤– LangChain Chat Models æ‡‰ç”¨ç¨‹å¼
        
        é€™æ˜¯ä¸€å€‹æ•´åˆå¤šç¨® AI æ¨¡å‹çš„å°è©±ä»‹é¢ï¼Œæ”¯æ´ï¼š
        - ğŸ”„ **å¤šæ¨¡å‹åˆ‡æ›**ï¼šOllamaã€Geminiã€OpenAIã€Anthropic
        - ğŸ§  **å°è©±è¨˜æ†¶**ï¼šä¿æŒå®Œæ•´çš„å°è©±ä¸Šä¸‹æ–‡
        - âš™ï¸ **ç³»çµ±è¨Šæ¯è¨­å®š**ï¼šè‡ªè¨‚ AI çš„è¡Œç‚º
        - ğŸ“¤ **å°è©±åŒ¯å‡º**ï¼šä¿å­˜å°è©±è¨˜éŒ„
        """)
        
        with gr.Row():
            with gr.Column(scale=4):  # å¢åŠ å°è©±å€åŸŸçš„æ¯”ä¾‹
                # èŠå¤©ä»‹é¢
                chatbot = gr.Chatbot(
                    label="ğŸ’¬ å°è©±ä»‹é¢",
                    height=600,  # å¢åŠ é«˜åº¦
                    show_label=True,
                    container=True,
                    type="messages"
                )
                
                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...",
                        label="è¼¸å…¥è¨Šæ¯",
                        lines=2,
                        scale=4
                    )
                    send_btn = gr.Button("ğŸ“¤ ç™¼é€", variant="primary", scale=1)
                
                with gr.Row():
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å°è©±", variant="secondary")
                    export_btn = gr.Button("ğŸ“¤ åŒ¯å‡ºå°è©±", variant="secondary")
            
            with gr.Column(scale=2):  # èª¿æ•´å³å´é¢æ¿æ¯”ä¾‹
                # æ¨¡å‹é¸æ“‡
                gr.Markdown("### âš™ï¸ æ¨¡å‹è¨­å®š")
                model_dropdown = gr.Dropdown(
                    choices=available_models,
                    value=chat_manager.current_model,
                    label="é¸æ“‡æ¨¡å‹",
                    interactive=True
                )
                
                # ç³»çµ±è¨Šæ¯è¨­å®š
                gr.Markdown("### ğŸ¯ ç³»çµ±è¨Šæ¯")
                system_msg_input = gr.Textbox(
                    value=chat_manager.system_message,
                    label="ç³»çµ±è¨Šæ¯",
                    lines=3,
                    placeholder="è¨­å®š AI çš„è¡Œç‚ºå’Œè§’è‰²..."
                )
                update_system_btn = gr.Button("ğŸ’¾ æ›´æ–°ç³»çµ±è¨Šæ¯", variant="secondary")
                
                # æ¨¡å‹è³‡è¨Š
                gr.Markdown("### ğŸ“Š æ¨¡å‹è³‡è¨Š")
                model_info_output = gr.Markdown(get_model_info())
                
                # åŒ¯å‡ºçµæœ
                gr.Markdown("### ğŸ“¤ åŒ¯å‡ºçµæœ")
                export_output = gr.Textbox(
                    label="åŒ¯å‡ºçš„å°è©±",
                    lines=10,
                    interactive=False
                )
        
        # äº‹ä»¶è™•ç†
        def update_model_info():
            return get_model_info()
        
        def handle_model_change(model_name):
            result = chat_manager.set_model(model_name)
            return result, update_model_info()
        
        def handle_export():
            return export_conversation()
        
        def handle_system_update(system_msg):
            result = chat_manager.set_system_message(system_msg)
            return result
        
        # ç¶å®šäº‹ä»¶
        send_btn.click(
            chat_function,
            inputs=[msg_input, chatbot, model_dropdown],
            outputs=[chatbot, msg_input]
        )
        
        msg_input.submit(
            chat_function,
            inputs=[msg_input, chatbot, model_dropdown],
            outputs=[chatbot, msg_input]
        )
        
        clear_btn.click(
            clear_chat,
            outputs=[chatbot, msg_input]
        )
        
        export_btn.click(
            handle_export,
            outputs=[export_output]
        )
        
        model_dropdown.change(
            handle_model_change,
            inputs=[model_dropdown],
            outputs=[gr.Textbox(visible=False), model_info_output]
        )
        
        update_system_btn.click(
            handle_system_update,
            inputs=[system_msg_input],
            outputs=[gr.Textbox(visible=False)]
        )
    
    return interface

def find_free_port(start_port=7860, max_port=7900):
    """å°‹æ‰¾å¯ç”¨çš„ç«¯å£"""
    for port in range(start_port, max_port):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return port
        except OSError:
            continue
    return None

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹• LangChain Chat Models Gradio æ‡‰ç”¨ç¨‹å¼...")
    
    # æª¢æŸ¥å¯ç”¨æ¨¡å‹
    available_models = chat_manager.get_available_models()
    if not available_models:
        print("âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼è«‹æª¢æŸ¥ï¼š")
        print("   1. Ollama æ˜¯å¦æ­£åœ¨é‹è¡Œ")
        print("   2. API é‡‘é‘°æ˜¯å¦æ­£ç¢ºè¨­å®š")
        print("   3. ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(available_models)} å€‹å¯ç”¨æ¨¡å‹:")
    for model in available_models:
        print(f"   - {model}")
    
    # å°‹æ‰¾å¯ç”¨ç«¯å£
    free_port = find_free_port()
    if not free_port:
        print("âŒ ç„¡æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ (7860-7900)")
        return
    
    print(f"ğŸŒ ä½¿ç”¨ç«¯å£: {free_port}")
    
    # å»ºç«‹ä¸¦å•Ÿå‹•ä»‹é¢
    interface = create_gradio_interface()
    
    print("ğŸŒ å•Ÿå‹• Web ä»‹é¢...")
    try:
        interface.launch(
            server_name="127.0.0.1",
 ##           server_name="0.0.0.0",            
            server_port=free_port,
            share=False,
            show_error=True
        )
    except Exception as e:
        print(f"âŒ å•Ÿå‹•å¤±æ•—: {e}")
        print("ğŸ’¡ è«‹æª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ‡‰ç”¨ç¨‹å¼ä½”ç”¨ç«¯å£")

if __name__ == "__main__":
    main()